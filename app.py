import streamlit as st
import os
from dotenv import load_dotenv
from io import BytesIO

from utils.auth import check_auth
from utils.pdf_loader import load_pdf_text
from utils.chunk_embed import embed_and_store_chunks
from utils.qa_chain import create_qa_chain
from utils.cache_utils import get_file_hash, load_vectorstore, save_vectorstore

# ğŸ” Authenticate user first
if not check_auth():
    st.stop()

# ğŸ”§ Load environment variables
load_dotenv()

# ğŸ§  Session States
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# ğŸ–¥ï¸ Page Config
st.set_page_config(page_title="PDFQueryAI", layout="wide")
st.title("ğŸ“„ PDF Query AI â€” Ask Questions From PDFs")

# ğŸ“ Upload PDFs
uploaded_files = st.file_uploader("Upload one or more PDF files", type=["pdf"], accept_multiple_files=True)

if uploaded_files:
    all_text = ""
    combined_hash = ""

    for uploaded_file in uploaded_files:
        file_bytes = uploaded_file.read()
        uploaded_file.seek(0)  # Reset stream
        file_hash = get_file_hash(file_bytes)
        combined_hash += file_hash
        all_text += load_pdf_text(uploaded_file) + "\n"

    # ğŸ—ƒï¸ Vector DB Cache
    cache_path = os.path.join("cached", combined_hash)

    vectordb = load_vectorstore(combined_hash)
    if vectordb:
        st.success("âœ… Loaded cached vector DB")
    else:
        st.info("ğŸ”„ Embedding documentsâ€¦")
        vectordb = embed_and_store_chunks(all_text, persist_path=cache_path)
        save_vectorstore(combined_hash, vectordb)
        st.success("âœ… Vector DB created and cached!")

    # ğŸ¤– Create QA Chain
    st.session_state.qa_chain = create_qa_chain(vectordb)

# â“ Query
if st.session_state.qa_chain:
    query = st.text_input("Ask a question about the documents:")

    if query:
        result = st.session_state.qa_chain.invoke({
            "question": query,
            "chat_history": st.session_state.chat_history
        })

        answer = result["answer"]
        sources = result.get("source_documents", [])

        st.markdown("### ğŸ¤– Answer")
        st.markdown(answer)

        # ğŸ§  Save history
        st.session_state.chat_history.append(("You", query))
        st.session_state.chat_history.append(("AI", answer))

        # ğŸ§¾ Show source chunks
        if sources:
            st.markdown("## ğŸ“š Source Chunks Used")
            col1, col2 = st.columns(2)

            with col1:
                st.markdown("### ğŸ“‚ Uploaded Files")
                for f in uploaded_files:
                    st.markdown(f"- `{f.name}`")

            with col2:
                st.markdown("### ğŸ§  Relevant Chunks")
                for i, doc in enumerate(sources):
                    st.markdown(f"**Chunk {i+1}:**")
                    st.code(doc.page_content.strip(), language="text")

# ğŸ’¾ Chat History Viewer
if st.session_state.chat_history:
    with st.expander("ğŸ§  Chat History"):
        for speaker, msg in st.session_state.chat_history:
            st.markdown(f"**{speaker}:** {msg}")

    def convert_to_txt():
        return "\n".join(f"{s}: {m}" for s, m in st.session_state.chat_history)

    chat_bytes = convert_to_txt().encode("utf-8")
    st.download_button("ğŸ’¾ Download Chat History", chat_bytes, file_name="chat_history.txt", mime="text/plain")
